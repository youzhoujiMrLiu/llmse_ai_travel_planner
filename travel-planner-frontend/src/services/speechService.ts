// src/services/speechService.ts

/**
 * ç§‘å¤§è®¯é£è¯­éŸ³è¯†åˆ«æœåŠ¡ï¼ˆé€šè¿‡åç«¯ WebSocket ä»£ç†ï¼‰
 * âœ… å®‰å…¨: API Key å®Œå…¨åœ¨åç«¯,å‰ç«¯æ— æ³•è·å–
 */
export class XFYunSpeechRecognition {
  private ws: WebSocket | null = null
  private isRecording = false
  private mediaRecorder: MediaRecorder | null = null
  private audioContext: AudioContext | null = null
  private audioStream: MediaStream | null = null

  /**
   * å¼€å§‹è¯­éŸ³è¯†åˆ«
   */
  async startRecognition(
    onResult: (text: string, isFinal: boolean) => void,
    onError: (error: string) => void
  ): Promise<void> {
    if (this.isRecording) {
      console.warn('å·²ç»åœ¨å½•éŸ³ä¸­')
      return
    }

    try {
      // è·å–éº¦å…‹é£æƒé™
      const stream = await navigator.mediaDevices.getUserMedia({ audio: true })
      this.audioStream = stream
      
      // åˆ›å»ºAudioContext
      this.audioContext = new AudioContext({ sampleRate: 16000 })
      const source = this.audioContext.createMediaStreamSource(stream)
      
      // è¿æ¥åˆ°åç«¯ WebSocket ä»£ç†ï¼ˆè€Œä¸æ˜¯ç›´æ¥è¿æ¥ç§‘å¤§è®¯é£ï¼‰
      const backendWsUrl = 'ws://localhost:8080/api/speech/websocket'
      this.ws = new WebSocket(backendWsUrl)
      
      this.ws.onopen = () => {
        console.log('âœ… å·²è¿æ¥åˆ°åç«¯è¯­éŸ³è¯†åˆ«æœåŠ¡')
        this.isRecording = true
        
        // å‘é€å¼€å§‹ä¿¡å·ç»™åç«¯
        this.ws!.send(JSON.stringify({ type: 'start' }))
        
        // å¼€å§‹å½•éŸ³å¹¶å‘é€éŸ³é¢‘æ•°æ®
        this.startRecordingAndSend(source, stream)
      }
      
      this.ws.onmessage = (event) => {
        const data = JSON.parse(event.data)
        
        if (data.code !== 0) {
          onError(`è¯†åˆ«é”™è¯¯: ${data.message}`)
          this.stopRecognition()
          return
        }
        
        if (data.data && data.data.result) {
          const result = data.data.result
          let text = ''
          
          result.ws.forEach((ws: any) => {
            ws.cw.forEach((cw: any) => {
              text += cw.w
            })
          })
          
          const isFinal = data.data.status === 2
          onResult(text, isFinal)
          
          if (isFinal) {
            console.log('âœ… è¯†åˆ«å®Œæˆ')
            this.stopRecognition()
          }
        }
      }
      
      this.ws.onerror = (error) => {
        console.error('âŒ WebSocketé”™è¯¯:', error)
        onError('WebSocketè¿æ¥å¤±è´¥')
        this.stopRecognition()
      }
      
      this.ws.onclose = () => {
        console.log('WebSocketè¿æ¥å…³é—­')
        this.isRecording = false
      }
      
    } catch (error: any) {
      onError(error.message || 'æ— æ³•è·å–éº¦å…‹é£æƒé™')
    }
  }

  /**
   * å¼€å§‹å½•éŸ³å¹¶å‘é€éŸ³é¢‘æ•°æ®åˆ°åç«¯
   */
  private startRecordingAndSend(source: MediaStreamAudioSourceNode, stream: MediaStream): void {
    // åˆ›å»ºScriptProcessorå¤„ç†éŸ³é¢‘æ•°æ®
    const processor = this.audioContext!.createScriptProcessor(4096, 1, 1)
    
    source.connect(processor)
    processor.connect(this.audioContext!.destination)
    
    processor.onaudioprocess = (e) => {
      if (!this.isRecording || !this.ws || this.ws.readyState !== WebSocket.OPEN) {
        return
      }
      
      const inputData = e.inputBuffer.getChannelData(0)
      const outputData = new Int16Array(inputData.length)
      
      // è½¬æ¢ä¸º16ä½PCM
      for (let i = 0; i < inputData.length; i++) {
        const sample = inputData[i] || 0
        const s = Math.max(-1, Math.min(1, sample))
        outputData[i] = s < 0 ? s * 0x8000 : s * 0x7FFF
      }
      
      // ç›´æ¥å‘é€äºŒè¿›åˆ¶æ•°æ®ç»™åç«¯ï¼ˆåç«¯ä¼šè½¬å‘ç»™ç§‘å¤§è®¯é£ï¼‰
      this.ws!.send(outputData.buffer)
    }
    
    // åœæ­¢æ—¶æ–­å¼€
    stream.getTracks().forEach(track => {
      track.onended = () => {
        processor.disconnect()
      }
    })
  }

  /**
   * åœæ­¢è¯­éŸ³è¯†åˆ«
   */
  stopRecognition(): void {
    console.log('ğŸ›‘ åœæ­¢å½•éŸ³')
    
    // åœæ­¢éŸ³é¢‘æµ
    if (this.audioStream) {
      this.audioStream.getTracks().forEach(track => track.stop())
      this.audioStream = null
    }
    
    if (this.ws && this.ws.readyState === WebSocket.OPEN) {
      // å‘é€ç»“æŸä¿¡å·ç»™åç«¯
      this.ws.send(JSON.stringify({ type: 'end' }))
      
      this.ws.close()
    }
    
    this.isRecording = false
    this.ws = null
    
    if (this.audioContext) {
      this.audioContext.close()
      this.audioContext = null
    }
  }
}

/**
 * ç®€åŒ–çš„è¯­éŸ³è¯†åˆ«æœåŠ¡ï¼ˆä½¿ç”¨ Web Speech APIï¼‰
 * ä½œä¸ºç§‘å¤§è®¯é£çš„å¤‡é€‰æ–¹æ¡ˆ
 */
export class WebSpeechRecognition {
  private recognition: any
  private isRecording = false
  private finalTranscript = ''

  constructor() {
    const SpeechRecognition = (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition
    if (!SpeechRecognition) {
      throw new Error('æµè§ˆå™¨ä¸æ”¯æŒè¯­éŸ³è¯†åˆ«')
    }

    this.recognition = new SpeechRecognition()
    this.recognition.lang = 'zh-CN'
    this.recognition.continuous = false  // âœ… æ”¹ä¸ºfalseï¼Œç‚¹å‡»åœæ­¢æ—¶æ‰ç»“æŸ
    this.recognition.interimResults = true  // æ˜¾ç¤ºä¸­é—´ç»“æœ
  }

  startRecognition(
    onResult: (text: string, isFinal: boolean) => void,
    onError: (error: string) => void
  ): void {
    if (this.isRecording) {
      console.warn('å·²ç»åœ¨å½•éŸ³ä¸­')
      return
    }

    this.isRecording = true
    this.finalTranscript = ''

    this.recognition.onresult = (event: any) => {
      let interimTranscript = ''
      
      for (let i = event.resultIndex; i < event.results.length; i++) {
        const transcript = event.results[i][0].transcript
        if (event.results[i].isFinal) {
          this.finalTranscript += transcript
          console.log('ğŸ“ æœ€ç»ˆç‰‡æ®µ:', transcript)
        } else {
          interimTranscript += transcript
        }
      }
      
      const fullText = this.finalTranscript + interimTranscript
      console.log('ğŸ¤ è¯†åˆ«ç»“æœ:', fullText, '(æœ€ç»ˆ:', this.finalTranscript, ', ä¸´æ—¶:', interimTranscript, ')')
      onResult(fullText, false)
    }

    this.recognition.onend = () => {
      console.log('ğŸ è¯†åˆ«ç»“æŸï¼Œæœ€ç»ˆæ–‡æœ¬:', this.finalTranscript, 'å½•éŸ³çŠ¶æ€:', this.isRecording)
      // è§¦å‘æœ€ç»ˆç»“æœ
      const finalText = this.finalTranscript.trim()
      if (finalText) {
        onResult(finalText, true)
      }
      this.isRecording = false
    }

    this.recognition.onerror = (event: any) => {
      console.error('âŒ è¯†åˆ«é”™è¯¯:', event.error)
      if (event.error !== 'no-speech' && event.error !== 'aborted') {
        onError('è¯†åˆ«å¤±è´¥: ' + event.error)
      }
      this.isRecording = false
    }

    try {
      this.recognition.start()
      console.log('ğŸ¤ å¼€å§‹è¯­éŸ³è¯†åˆ« (Web Speech API)')
    } catch (error: any) {
      onError('å¯åŠ¨è¯†åˆ«å¤±è´¥: ' + error.message)
      this.isRecording = false
    }
  }

  stopRecognition(): void {
    if (this.isRecording) {
      console.log('ğŸ›‘ åœæ­¢è¯­éŸ³è¯†åˆ«ï¼Œå½“å‰æ–‡æœ¬:', this.finalTranscript)
      // ä¸è¦åœ¨è¿™é‡Œè®¾ç½®isRecording = falseï¼Œè®©onendäº‹ä»¶å¤„ç†
      this.recognition.stop()
    }
  }
}
